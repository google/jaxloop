{
  "cells": [
    {
      "metadata": {
        "id": "8D1R_G_Cl40_"
      },
      "cell_type": "markdown",
      "source": [
        "# SimpleTrainer (Intermediate)"
      ]
    },
    {
      "metadata": {
        "id": "wH_E6OWldlfT"
      },
      "cell_type": "markdown",
      "source": [
        "This notebook provides examples of more complex customizations using the Jaxloop SimpleTrainer."
      ]
    },
    {
      "metadata": {
        "id": "bNFAa38dmH0G"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ]
    },
    {
      "metadata": {
        "id": "hUELesMm7Ra4"
      },
      "cell_type": "code",
      "source": [
        "from colabtools import adhoc_import\n",
        "from flax import linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.random import PRNGKey\n",
        "import optax\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "with adhoc_import.Google3SubmittedChangelist():\n",
        "  from jaxloop.trainers import simple_trainer\n",
        "  from jaxloop.trainers import simple_step\n",
        "\n",
        "from jaxloop import step\n",
        "from jaxloop import types\n",
        "from typing import Tuple, Mapping, Sequence"
      ],
      "outputs": [],
      "execution_count": 1
    },
    {
      "metadata": {
        "id": "9Ou5ybOXmB9q"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "metadata": {
        "id": "zl5x4qT98_Uf"
      },
      "cell_type": "code",
      "source": [
        "def get_datasets(batch_size: int):\n",
        "  \"\"\"Loads the MNIST dataset.\n",
        "\n",
        "  Returns:\n",
        "    train_ds: A tf.data.Dataset object containing the training data.\n",
        "    test_ds: A tf.data.Dataset object containing the test data.\n",
        "  \"\"\"\n",
        "  train_ds = tfds.load('mnist', split='train')\n",
        "  test_ds = tfds.load('mnist', split='test')\n",
        "\n",
        "  train_ds = train_ds.map(\n",
        "      lambda sample: {\n",
        "          'input_features': tf.cast(sample['image'], tf.float32) / 255.0,\n",
        "          'label': sample['label'],\n",
        "      }\n",
        "  )\n",
        "  test_ds = test_ds.map(\n",
        "      lambda sample: {\n",
        "          'input_features': tf.cast(sample['image'], tf.float32) / 255.0,\n",
        "          'label': sample['label'],\n",
        "      }\n",
        "  )\n",
        "\n",
        "  train_ds = train_ds.repeat().shuffle(1024)\n",
        "  train_ds = train_ds.batch(batch_size, drop_remainder=True).prefetch(1)\n",
        "\n",
        "  test_ds = test_ds.shuffle(1024)\n",
        "  test_ds = test_ds.batch(batch_size, drop_remainder=True).prefetch(1)\n",
        "\n",
        "  return train_ds, test_ds"
      ],
      "outputs": [],
      "execution_count": 2
    },
    {
      "metadata": {
        "id": "HnRSkRiXmUDF"
      },
      "cell_type": "markdown",
      "source": [
        "## Model"
      ]
    },
    {
      "metadata": {
        "id": "EnXiLOWq96Cf"
      },
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "  \"\"\"A CNN model with 2 convolutional layers, 2 pooling layers, and a dense layer.\"\"\"\n",
        "\n",
        "  num_classes: int = 10\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, input_features: jax.Array, train: bool = False):\n",
        "    x = input_features\n",
        "    x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
        "    x = nn.relu(x)\n",
        "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
        "    x = nn.Conv(features=128, kernel_size=(3, 3))(x)\n",
        "    x = nn.relu(x)\n",
        "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
        "\n",
        "    x = x.reshape((x.shape[0], -1))\n",
        "    x = nn.Dense(features=256)(x)\n",
        "    x = nn.relu(x)\n",
        "    logits = nn.Dense(features=self.num_classes)(x)\n",
        "\n",
        "    return logits"
      ],
      "outputs": [],
      "execution_count": 3
    },
    {
      "metadata": {
        "id": "8A9Rnq16mc1H"
      },
      "cell_type": "markdown",
      "source": [
        "## Customizing the Step Class\n",
        "The default SimpleStep class uses the MSE loss function. We extend the SimpleStep class and override its loss function."
      ]
    },
    {
      "metadata": {
        "id": "avs5iMAHziKb"
      },
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss_fn(logits: jax.Array, labels: jax.Array) -\u003e jax.Array:\n",
        "  \"\"\"Computes the softmax cross-entropy loss with integer labels.\"\"\"\n",
        "  return jnp.mean(\n",
        "      optax.softmax_cross_entropy_with_integer_labels(\n",
        "          logits=logits, labels=labels\n",
        "      )\n",
        "  )\n",
        "\n",
        "\n",
        "class CrossEntropyStep(simple_step.SimpleStep):\n",
        "  \"\"\"A step that uses cross-entropy loss.\"\"\"\n",
        "\n",
        "  # Recall that the SimpleStep expects the key for output features to be \"output_features\".\n",
        "  # We will override _get_output_features to instead extract output_features with the key \"label\".\n",
        "  def _get_output_features(self, batch: dict) -\u003e jax.Array:\n",
        "    \"\"\"Extracts the labels from the batch to be used as the ground truth.\"\"\"\n",
        "    return batch[\"label\"]\n",
        "\n",
        "  def loss_fn(\n",
        "      self,\n",
        "      output_features_pred: jax.Array,\n",
        "      true_output_features: jax.Array,\n",
        "  ) -\u003e jax.Array:\n",
        "    \"\"\"Computes the cross-entropy loss between the predicted logits and the true labels.\n",
        "\n",
        "    Args:\n",
        "      output_features_pred: The predicted logits from the model.\n",
        "      true_output_features: The true integer labels.\n",
        "\n",
        "    Returns:\n",
        "      The cross-entropy loss.\n",
        "    \"\"\"\n",
        "    return cross_entropy_loss_fn(output_features_pred, true_output_features)"
      ],
      "outputs": [],
      "execution_count": 4
    },
    {
      "metadata": {
        "id": "gC2E5zN3nIfQ"
      },
      "cell_type": "markdown",
      "source": [
        "## Initialization \u0026 Training"
      ]
    },
    {
      "metadata": {
        "id": "h3tKlb7a-pZ0"
      },
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "\n",
        "NUM_EPOCHS = 10\n",
        "STEPS_PER_EPOCH = 125\n",
        "\n",
        "num_train_steps = STEPS_PER_EPOCH * NUM_EPOCHS"
      ],
      "outputs": [],
      "execution_count": 5
    },
    {
      "metadata": {
        "id": "xM0cgjIeix9U"
      },
      "cell_type": "code",
      "source": [
        "train_ds, test_ds = get_datasets(batch_size)"
      ],
      "outputs": [],
      "execution_count": 6
    },
    {
      "metadata": {
        "id": "C_E2X5jY_T_l"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"We get the shape of our input_features from the train_ds with .element_spec and\n",
        "\n",
        "convert the resulting TensorShape to a Python tuple. Then we get the data type\n",
        "of\n",
        "our input_features and convert to a numpy data type. The resulting tuple is\n",
        "the input_features for the BATCH_SPEC. The same process is repeated for the\n",
        "label.\n",
        "\"\"\"\n",
        "\n",
        "BATCH_SPEC: Mapping[str, Tuple[Sequence[int], type]] = {\n",
        "    \"input_features\": (\n",
        "        tuple(train_ds.element_spec[\"input_features\"].shape),\n",
        "        train_ds.element_spec[\"input_features\"].dtype.as_numpy_dtype,\n",
        "    ),\n",
        "    \"label\": (\n",
        "        tuple(train_ds.element_spec[\"label\"].shape),\n",
        "        train_ds.element_spec[\"label\"].dtype.as_numpy_dtype,\n",
        "    ),\n",
        "}"
      ],
      "outputs": [],
      "execution_count": 7
    },
    {
      "metadata": {
        "id": "Mxc3hob2BW9z"
      },
      "cell_type": "code",
      "source": [
        "CNN_MODEL = SimpleCNN()\n",
        "OPTIMIZER = optax.adam(learning_rate)\n",
        "\n",
        "prng_seed = 0\n",
        "prng = PRNGKey(prng_seed)\n",
        "BASE_PRNG = {\"params\": prng}\n",
        "\n",
        "CHECKPOINTING_CONFIG = None\n",
        "\n",
        "# Create an instance of the CrossEntropyStep class\n",
        "CROSS_ENTROPY_STEP = CrossEntropyStep(\n",
        "    base_prng=BASE_PRNG,\n",
        "    model=CNN_MODEL,\n",
        "    optimizer=OPTIMIZER,\n",
        "    train=True,\n",
        ")\n",
        "\n",
        "# Create the trainer\n",
        "trainer = simple_trainer.SimpleTrainer(\n",
        "    model=CNN_MODEL,\n",
        "    epochs=NUM_EPOCHS,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    batch_spec=BATCH_SPEC,\n",
        "    step_class=CROSS_ENTROPY_STEP.__class__, # Pass a reference to the type of the instance\n",
        "    optimizer=OPTIMIZER,\n",
        "    base_prng=BASE_PRNG,\n",
        "    log_num_params=True,\n",
        "    checkpointing_config=CHECKPOINTING_CONFIG,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 8
    },
    {
      "metadata": {
        "id": "4Tiqzglov5ks"
      },
      "cell_type": "code",
      "source": [
        "train_outputs = trainer.train(train_ds.as_numpy_iterator())\n",
        "trained_model_state = trainer.model_state"
      ],
      "outputs": [],
      "execution_count": 9
    },
    {
      "metadata": {
        "id": "45Ut94HInMDW"
      },
      "cell_type": "markdown",
      "source": [
        "## Testing \u0026 Visualization"
      ]
    },
    {
      "metadata": {
        "id": "kFTMlBeM2ZoJ"
      },
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def predict_batch(model_state: types.TrainState, batch_input_features: jax.Array) -\u003e jax.Array:\n",
        "  logits = model_state.apply_fn({'params': model_state.params}, batch_input_features, train=False)\n",
        "  return logits\n",
        "\n",
        "all_test_losses = []\n",
        "all_test_accuracies = []\n",
        "total_samples = 0\n",
        "total_correct_predictions = 0\n",
        "\n",
        "test_batch = None\n",
        "logits = None\n",
        "\n",
        "for test_batch in test_ds.as_numpy_iterator():\n",
        "  batch_input_features = test_batch['input_features']\n",
        "  batch_labels = test_batch['label']\n",
        "\n",
        "  logits = predict_batch(trained_model_state, batch_input_features)\n",
        "\n",
        "  batch_loss = cross_entropy_loss_fn(logits, batch_labels)\n",
        "  all_test_losses.append(batch_loss.item())\n",
        "\n",
        "  predicted_classes = jnp.argmax(logits, axis=-1)\n",
        "  correct_predictions_in_batch = jnp.sum(predicted_classes == batch_labels)\n",
        "  all_test_accuracies.append(correct_predictions_in_batch.item() / batch_labels.shape[0])\n",
        "\n",
        "  total_correct_predictions += correct_predictions_in_batch.item()\n",
        "  total_samples += batch_labels.shape[0]\n",
        "\n",
        "average_test_loss = sum(all_test_losses) / len(all_test_losses)\n",
        "average_test_accuracy = total_correct_predictions / total_samples\n",
        "\n",
        "print(f\"Average Test Loss: {average_test_loss}\")\n",
        "print(f\"Average Test Accuracy: {average_test_accuracy}\")\n",
        "\n",
        "# Store the last batch for visualization.\n",
        "example_test_batch = test_batch\n",
        "example_logits = logits\n",
        "example_preds = jnp.argmax(example_logits, axis=-1)"
      ],
      "outputs": [],
      "execution_count": 10
    },
    {
      "metadata": {
        "id": "VGTi8PFk3bkA"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axs = plt.subplots(5, 5, figsize=(12, 12))\n",
        "fig.suptitle('MNIST Test Set Predictions', fontsize=16)\n",
        "\n",
        "for i, ax in enumerate(axs.flatten()):\n",
        "    if i \u003c len(example_preds):\n",
        "        ax.imshow(jnp.squeeze(example_test_batch['input_features'][i]), cmap='gray')\n",
        "\n",
        "        true_label = example_test_batch['label'][i]\n",
        "        predicted_label = example_preds[i]\n",
        "        ax.set_title(f\"True: {true_label}\\nPred: {predicted_label}\",\n",
        "                     color='green' if true_label == predicted_label else 'red')\n",
        "\n",
        "        ax.axis('off')\n",
        "    else:\n",
        "        ax.axis('off')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": 11
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//third_party/py/jaxloop/g3doc:jaxloop_documentation_colab",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1LnyR4Eiq4FvC-_Q6ExI9AmgpvHO9fsJQ",
          "timestamp": 1749236652786
        },
        {
          "file_id": "1ghj_MmRWaMfNeG8ehhCZ3JagP_vJ3OMu",
          "timestamp": 1749233808221
        },
        {
          "file_id": "1oCWO6XwP7LnbMlJgrpOrFUz2wR2qoKYY",
          "timestamp": 1748463380266
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
