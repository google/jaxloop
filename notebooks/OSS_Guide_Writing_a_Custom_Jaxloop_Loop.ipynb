{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEmxc_JSPdVg"
      },
      "source": [
        "## Custom Jaxloop\n",
        "\n",
        "Jaxloop comprises five key parts: model, datasets, steps, inner loops, and outer loop. To construct an experiment using Jaxloop, follow these steps:\n",
        "\n",
        "1. **Define the Model:**  Specify the architecture and functionality of your machine learning model.\n",
        "2. **Define Datasets:**  Prepare your training and evaluation data, ensuring proper formatting and loading procedures.\n",
        "3. **Define Steps:**  Create individual functions for distinct operations, such as calculating loss, updating parameters, or evaluating metrics. In Keras terms, a step is similar to an \"iteration\" - a single forward and backward pass on a batch of data.\n",
        "4. **Define Inner Loops:**  Combine steps into iterative processes, like a training loop that updates model parameters over multiple batches of data or evaluation loops that assesses the performance of your model parameters. In Keras, an inner loop is analogous to an \"epoch\" - one complete pass through the entire dataset (or one pass of a given number of data samples).\n",
        "5. **Define and Invoke the Outer Loop:**  Orchestrate the overall experimental workflow, potentially encompassing multiple inner loops for training, evaluation, and hyperparameter tuning. In Keras, this is similar to the overall \"training loop\" that encompasses all epochs.\n",
        "\n",
        "Each of these components will be explored in detail below.\n",
        "\n",
        "\u003c!-- TODO(b/379344058) Descibe how to install Jaxloop and its dependencies using pip or link to installation instructions --\u003e\n",
        "**Note:** To run the code examples provided in this documentation, please install Jaxloop and its dependencies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUEX0FDL8RHg"
      },
      "source": [
        "# Model\n",
        "\n",
        "Jaxloop is compatible with models written using either [Flax Linen](http://shortn/_x49iy2sUHL) or the newer [Flax NNX](http://shortn/_wisT9XxhAv) API.\n",
        "\n",
        "For the purposes of this colab, we'll utilize a standard CNN example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGn16WFB2RmR"
      },
      "outputs": [],
      "source": [
        "from flax import linen as nn\n",
        "\n",
        "class CNN(nn.Module):\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
        "    x = nn.relu(x)\n",
        "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
        "    x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n",
        "    x = nn.relu(x)\n",
        "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
        "    x = x.reshape((x.shape[0], -1))\n",
        "    x = nn.Dense(features=256)(x)\n",
        "    x = nn.relu(x)\n",
        "    x = nn.Dense(features=10)(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTU2Ds_jI7oK"
      },
      "source": [
        "# Datasets\n",
        "\n",
        "Jaxloop requires the training dataset to be provided as an **iterator**. This is essential because training involves iterating over different subsets of the dataset.\n",
        "\n",
        "Here's why:\n",
        "\n",
        "* **Statefulness:** Iterators maintain an internal state, allowing them to keep track of their position within the dataset. This ensures that each training epoch covers distinct data subsets.\n",
        "* **Lazy Evaluation:** Iterators generate data on demand, rather than loading the entire dataset into memory at once. This is crucial for handling large datasets efficiently.\n",
        "\n",
        "Jaxloop offers flexibility in how you provide the training data:\n",
        "\n",
        "* **Generators:** You can use Python generators to create custom data iterators.\n",
        "* **Data Ingestion Frameworks:**  Integrate seamlessly with popular frameworks like [TF Data](http://shortn/_oX61MDRjYv) or [PyGrian](http://shortn/_aCGKZaAhTI) for advanced data loading and preprocessing.\n",
        "* **Automatic Conversion:** If you happen to provide a non-iterator iterable (like a list), Jaxloop will automatically convert it into an iterator internally.\n",
        "\n",
        "In contrast to the training dataset, the evaluation dataset in Jaxloop must be **an iterable object, not an iterator**. This is because evaluation typically uses the same, fixed subset of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrefAoMtKhZK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "def mnist_datasets(batch_size, data_dir):\n",
        "  def map_fn(x):\n",
        "    return {\n",
        "        'image': tf.cast(x['image'], tf.float32) / 255.0,\n",
        "        'label': tf.cast(x['label'], tf.int32),\n",
        "    }\n",
        "\n",
        "  train_dir = os.path.join(data_dir, 'train')\n",
        "  test_dir = os.path.join(data_dir, 'test')\n",
        "  train_ds = (\n",
        "      tfds.load('mnist', data_dir=train_dir, split='train', shuffle_files=True)\n",
        "      .map(map_fn)\n",
        "      .batch(batch_size, drop_remainder=True)\n",
        "      .prefetch(tf.data.AUTOTUNE)\n",
        "      .repeat()\n",
        "  )\n",
        "  eval_ds = (\n",
        "      tfds.load('mnist', data_dir=test_dir, split='test', shuffle_files=False)\n",
        "      .map(map_fn)\n",
        "      .batch(batch_size, drop_remainder=True)\n",
        "      .prefetch(tf.data.AUTOTUNE)\n",
        "      .cache()\n",
        "  )\n",
        "  return train_ds, eval_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt30ZnAx8Tsk"
      },
      "source": [
        "# Steps\n",
        "\n",
        "In Jaxloop, a \"step\" refers to a modular unit of computation that processes a batch of data from your dataset. Each step is defined as a class inheriting from `jaxloop.Step`, and its core logic resides within the `run` function.\n",
        "\n",
        "Here's a breakdown of key aspects:\n",
        "\n",
        "* **`run` function:** This is the heart of a Jaxloop step. You, as the user, are responsible for implementing this function to define how a batch of data is processed. This could involve anything from computing the loss and gradients to updating model parameters or calculating evaluation metrics.\n",
        "* **Train vs. Eval Steps:** A Jaxloop step can be designated as either a training step or an evaluation step. This distinction is controlled by a boolean input parameter. The implementation of the `run` function will typically differ between train and eval steps, reflecting the different tasks they perform.\n",
        "* **Outputs:** The `run` function returns two values:\n",
        "    * `train_state`:  An object containing the current state of your model and optimizer.\n",
        "    * `output`: A dictionary (with string keys) where you can store any data relevant to the step's execution, such as loss values, accuracy metrics, gradients, or anything else you need to track.\n",
        "* **JIT Compilation and Sharding:** To maximize performance, Jaxloop automatically compiles your `run` function using Just-In-Time (JIT) compilation and distributes the computation across multiple devices (if available) using a technique called sharding. The specific sharding strategy is determined by a \"partitioner\" object, which we'll discuss in more detail later.\n",
        "* **`begin` and `end` functions:**  Jaxloop provides optional `begin` and `end` functions that you can implement within your step class. These functions allow you to perform pre-processing or post-processing operations on the `train_state` and `output` before and after the `run` function is executed.\n",
        "\n",
        "By encapsulating different stages of your training and evaluation pipeline into these modular steps, Jaxloop promotes code organization, reusability, and flexibility in designing your experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKGlxM-W8VvN"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jaxloop import step as step_lib\n",
        "from jaxloop import types\n",
        "import optax\n",
        "import tensorflow as tf\n",
        "\n",
        "Batch = types.Batch\n",
        "Output = types.Output\n",
        "State = types.TrainState\n",
        "Step = step_lib.Step\n",
        "\n",
        "class MnistStep(Step):\n",
        "\n",
        "  def begin(self, state: State, batch: Batch) -\u003e tuple[State, Batch]:\n",
        "    if isinstance(batch['image'], tf.Tensor):\n",
        "      batch['image'] = batch['image'].numpy()\n",
        "    if isinstance(batch['label'], tf.Tensor):\n",
        "      batch['label'] = batch['label'].numpy()\n",
        "    return state, batch\n",
        "\n",
        "  def run(self, state: State, batch: Batch) -\u003e tuple[State, Optional[Output]]:\n",
        "    images, labels = batch['image'], batch['label']\n",
        "\n",
        "    def loss_fn(params):\n",
        "      logits = state.apply_fn({'params': params}, images)\n",
        "      one_hot = jax.nn.one_hot(labels, 10)\n",
        "      loss = jnp.mean(optax.softmax_cross_entropy(logits, one_hot))\n",
        "      return loss, logits\n",
        "\n",
        "    if self.train:\n",
        "      grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "      (loss, logits), grads = grad_fn(state.params)\n",
        "      state = state.apply_gradients(grads=grads)\n",
        "    else:\n",
        "      loss, logits = loss_fn(state.params)\n",
        "\n",
        "    accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == labels)\n",
        "    return state, {'loss': loss, 'accuracy': accuracy}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow-lClYf9qk0"
      },
      "source": [
        "Jaxloop Step uses partitioners to jit and shard [batch data](http://shortn/_CYzkbhgGP2), [model initialization](http://shortn/_6IvLhMS8nf), and [step run](http://shortn/_B0hgnwfG4G).\n",
        "\n",
        "\u003c!-- TODO(b/379340967) update link to partitioner explanation --\u003e\n",
        "\u003c!-- Please refer to this [section]() for more in-depth explanation of parititioners. --\u003e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSba6ket9p46"
      },
      "outputs": [],
      "source": [
        "from jax import sharding\n",
        "from jaxloop import partition\n",
        "from jax.experimental import mesh_utils\n",
        "\n",
        "Mesh = sharding.Mesh\n",
        "\n",
        "num_devices = len(jax.devices())\n",
        "mesh = Mesh(mesh_utils.create_device_mesh((num_devices,)), ('data',))\n",
        "partitioner = partition.DataParallelPartitioner(mesh=mesh, data_axis='data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tAK1xM0RKNl"
      },
      "source": [
        "# Inner Loops\n",
        "\n",
        "Jaxloop provides \"inner loops\" to handle the iterative execution of your experiment steps, whether for training or evaluation.\n",
        "\n",
        "Here's how they work:\n",
        "\n",
        "* **Train Loop:** This loop focuses on training your model. It repeatedly executes your defined training step for a specified number of iterations (`train_loop_steps`). If `train_loop_steps` is not provided, the loop will iterate over the entire training dataset.\n",
        "* **Eval Loop:** This loop is designed for evaluating your model's performance. It runs your defined evaluation step for a set number of iterations (`eval_spec.num_steps`) or until the entire evaluation dataset has been processed.\n",
        "\n",
        "**Adding Functionality with Actions**\n",
        "\n",
        "Jaxloop allows you to incorporate custom actions at the beginning and end of each inner loop. These actions are essentially functions that perform specific tasks periodically during training or evaluation.\n",
        "\n",
        "* **Built-in Actions:** Jaxloop offers pre-built actions like:\n",
        "    * `SummaryAction`:  Used for logging summaries (e.g., metrics, visualizations) during training.\n",
        "    * `CheckpointAction`:  Handles saving model checkpoints at regular intervals.\n",
        "* **Custom Actions:** You can also define your own actions to perform any operations you need, such as learning rate scheduling, early stopping, or custom logging.\n",
        "\n",
        "We'll delve deeper into actions and how to use them effectively in a dedicated section later in this documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79VPC8EV5-ky"
      },
      "outputs": [],
      "source": [
        "from orbax import checkpoint\n",
        "from clu import metric_writers\n",
        "from jaxloop import actions\n",
        "from etils import epath\n",
        "\n",
        "import tempfile\n",
        "\n",
        "work_dir = tempfile.mkdtemp()\n",
        "work_dir = epath.Path(work_dir)\n",
        "\n",
        "ckpt_manager = checkpoint.CheckpointManager(\n",
        "    work_dir / 'checkpoints',\n",
        "    checkpoint.Checkpointer(checkpoint.PyTreeCheckpointHandler()),\n",
        "    checkpoint.CheckpointManagerOptions(max_to_keep=3),\n",
        ")\n",
        "metrics_writer = metric_writers.create_default_writer(\n",
        "    work_dir,\n",
        "    asynchronous=False,\n",
        ")\n",
        "\n",
        "ckpt_action = actions.CheckpointAction(ckpt_manager, interval=100)\n",
        "summary_action = actions.SummaryAction(metrics_writer, interval=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1VUi7E3R8Oo"
      },
      "source": [
        "# Outer Loop\n",
        "\n",
        "The outer loop in Jaxloop serves as the orchestrator of your entire training experiment. It combines the train and eval inner loops, running them repeatedly until a specified number of total training steps (`train_total_steps`) is reached.\n",
        "\n",
        "**Initiating the Training Process**\n",
        "\n",
        "To kickstart your training experiment using the Jaxloop outer loop, you'll need to provide the following:\n",
        "\n",
        "* **Initialized Model State:** Start by initializing your model's parameters and optimizer state. This forms the basis for your `train_step`.\n",
        "* **Training Dataset Iterator:**  Provide your training dataset in the form of an iterator, as explained earlier.\n",
        "* **`train_total_steps`:** Define the total number of steps you want the training process to run for. This determines the overall duration of the experiment.\n",
        "* **`train_loop_steps`:** Specify the number of steps to execute within each individual training loop.\n",
        "* **`eval_specs`:** Configure the evaluation phase by providing:\n",
        "    * The evaluation dataset.\n",
        "    * The interval (in terms of steps) at which you want to run the evaluation loop.\n",
        "\n",
        "By configuring these elements, you provide the outer loop with the necessary instructions to manage the training and evaluation processes effectively, ensuring your experiment runs smoothly and efficiently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8ZoNfNW6KR_"
      },
      "source": [
        "# Putting it all Together: A Custom Jaxloop Experiment\n",
        "\n",
        "The following code snippet demonstrates how to orchestrate a complete training experiment using Jaxloop, integrating all the components we've discussed – models, datasets, steps, inner loops, and the outer loop. This serves as the main driver for your custom Jaxloop experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tx-fSoDH1dF"
      },
      "outputs": [],
      "source": [
        "from jaxloop import eval_loop as eval_loop_lib\n",
        "from jaxloop import outer_loop as outer_loop_lib\n",
        "from jaxloop import train_loop as train_loop_lib\n",
        "import tempfile\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# 1. Define model.\n",
        "model = CNN()\n",
        "\n",
        "# 2. Define train and eval datasets.\n",
        "data_dir = tempfile.mkdtemp()\n",
        "train_ds, eval_ds = mnist_datasets(\n",
        "    batch_size=32, data_dir=data_dir\n",
        ")\n",
        "\n",
        "# 3. Define training and eval steps.\n",
        "train_step = MnistStep(\n",
        "    jax.random.PRNGKey(0),\n",
        "    model,\n",
        "    optimizer=optax.sgd(learning_rate=0.005, momentum=0.9),\n",
        "    partitioner=partitioner,\n",
        "    train=True,\n",
        ")\n",
        "eval_step = MnistStep(\n",
        "    jax.random.PRNGKey(0),\n",
        "    model,\n",
        "    partitioner=partitioner,\n",
        "    train=False,\n",
        ")\n",
        "\n",
        "# 4. Define inner loops.\n",
        "train_loop = train_loop_lib.TrainLoop(\n",
        "    train_step, end_actions=[summary_action, ckpt_action]\n",
        ")\n",
        "eval_loop = eval_loop_lib.EvalLoop(eval_step, end_actions=[summary_action])\n",
        "\n",
        "# 5. Define and invoke outer loop.\n",
        "outer_loop = outer_loop_lib.OuterLoop(\n",
        "    train_loop=train_loop, eval_loops=[eval_loop]\n",
        ")\n",
        "state, outputs = outer_loop(\n",
        "    train_step.initialize_model([1, 28, 28, 1]),\n",
        "    train_dataset=train_ds.as_numpy_iterator(),\n",
        "    train_total_steps=100,\n",
        "    train_loop_steps=10,\n",
        "    eval_specs=[\n",
        "        outer_loop_lib.EvalSpec(\n",
        "            dataset=tfds.as_numpy(eval_ds), num_steps=100\n",
        "        )\n",
        "    ],\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//third_party/py/jaxloop/g3doc:jaxloop_documentation_colab",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/third_party/py/jaxloop/notebooks/OSS_Guide_Writing_a_Custom_Jaxloop_Loop.ipynb",
          "timestamp": 1731713002076
        },
        {
          "file_id": "1vKi88iF8GfonwFbCncmPqq1jIBd9Xm1y",
          "timestamp": 1731540993959
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
